{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6192386,"sourceType":"datasetVersion","datasetId":3554542}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndataset_train_path = '/kaggle/input/amazon-reviews-multi/train.csv'\ndataset_val_path = '/kaggle/input/amazon-reviews-multi/validation.csv'\ndataset_test_path = '/kaggle/input/amazon-reviews-multi/test.csv'\n\ntrain_data = pd.read_csv(dataset_train_path)\nval_data = pd.read_csv(dataset_val_path)\ntest_data = pd.read_csv(dataset_test_path)\n\nprint(\"Train Dataset Information:\")\ntrain_data.info()\nprint(\"\\n\")\n\nprint(\"Validation Dataset Information:\")\nval_data.info()\nprint(\"\\n\")\n\nprint(\"Test Dataset Information:\")\ntest_data.info()\nprint(\"\\n\")\n\n# Print all column names\nprint(\"Columns in the datasets:\")\nprint(train_data.columns.tolist())\nprint(\"\\n\")\n\n# Print the number of unique values in each column for train dataset\nprint(\"Number of unique values in each column (Train Dataset):\")\nunique_values_train = train_data.nunique()\nprint(unique_values_train)\nprint(\"\\n\")\n\n# Print the number of unique values in each column for validation dataset\nprint(\"Number of unique values in each column (Validation Dataset):\")\nunique_values_val = val_data.nunique()\nprint(unique_values_val)\nprint(\"\\n\")\n\n# Print the number of unique values in each column for test dataset\nprint(\"Number of unique values in each column (Test Dataset):\")\nunique_values_test = test_data.nunique()\nprint(unique_values_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T16:52:04.470179Z","iopub.execute_input":"2024-12-30T16:52:04.470549Z","iopub.status.idle":"2024-12-30T16:52:24.626532Z","shell.execute_reply.started":"2024-12-30T16:52:04.470519Z","shell.execute_reply":"2024-12-30T16:52:24.625081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Bidirectional\n\n\ntrain_path = '/kaggle/input/amazon-reviews-multi/train.csv'\nval_path = '/kaggle/input/amazon-reviews-multi/validation.csv'\ntest_path = '/kaggle/input/amazon-reviews-multi/test.csv'\n\ntrain_data = pd.read_csv(train_path)\nval_data = pd.read_csv(val_path)\ntest_data = pd.read_csv(test_path)\n\n# Filter reviews in train, val, test\n#train_data = train_data[train_data['language'] == 'de']\n#val_data = val_data[val_data['language'] == 'de']\n#test_data = test_data[test_data['language'] == 'de']\n\n# Combine review_body and review_title\nfor data in [train_data, val_data, test_data]:\n    data['review_text'] = data['review_body'].fillna('') + ' ' + data['review_title'].fillna('')\n\n# Sentiment labels\ndef sentiment_label(stars):\n    if stars > 3:\n        return 1  # Positive\n    else:\n        return 0  # Negative\n\nfor data in [train_data, val_data, test_data]:\n    data['sentiment'] = data['stars'].apply(sentiment_label)\n\n# Tokenization\ntokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_data['review_text'])\n\n# Convert text to sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_data['review_text'])\nval_sequences = tokenizer.texts_to_sequences(val_data['review_text'])\ntest_sequences = tokenizer.texts_to_sequences(test_data['review_text'])\n\n# Pad sequences\nmax_len = 200\ntrain_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\nval_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\ntest_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n\ntrain_labels = train_data['sentiment'].values\nval_labels = val_data['sentiment'].values\ntest_labels = test_data['sentiment'].values\n\n\n##########################################\n#### LSTM Sentiment (0 / 1)\n##########################################\n\nmodel = Sequential([\n    Embedding(input_dim=20000, output_dim=128, input_length=max_len),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)),\n    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(2, activation='softmax')  # 2 classes: positive, negative\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(\n    train_padded, train_labels,\n    epochs=3,\n    batch_size=256,\n    validation_data=(val_padded, val_labels),\n    verbose=1\n)\n\nloss, accuracy = model.evaluate(test_padded, test_labels)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n##########################################\n#### Predict whole dataset based on LSTM\n##########################################\n\ntrain_pred = np.argmax(model.predict(train_padded), axis=1)\nval_pred = np.argmax(model.predict(val_padded), axis=1)\ntest_pred = np.argmax(model.predict(test_padded), axis=1)\n\ntrain_data['sentiment_lstm'] = train_pred\nval_data['sentiment_lstm'] = val_pred\ntest_data['sentiment_lstm'] = test_pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T00:46:54.962050Z","iopub.execute_input":"2024-12-28T00:46:54.962445Z","execution_failed":"2024-12-28T00:54:17.293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, SpatialDropout1D, Bidirectional\nfrom sklearn.model_selection import train_test_split\n\n##########################################\n#### LSTM Sentiment Score Value between 0 and 1\n##########################################\nmodel = Sequential([\n    Embedding(input_dim=20000, output_dim=128, input_length=max_len),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)),\n    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n    Dense(32, activation='relu'),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid')  # Single neuron with sigmoid activation for probability output\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(\n    train_padded, train_labels,\n    epochs=3,\n    batch_size=256,\n    validation_data=(val_padded, val_labels),\n    verbose=1\n)\n\nloss, accuracy = model.evaluate(test_padded, test_labels)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n##########################################\n#### Predict whole dataset based on LSTM\n##########################################\ntrain_pred = model.predict(train_padded)\nval_pred = model.predict(val_padded)\ntest_pred = model.predict(test_padded)\n\ntrain_data['sentiment_score_lstm'] = train_pred\nval_data['sentiment_score_lstm'] = val_pred\ntest_data['sentiment_score_lstm'] = test_pred\n\ncombined_data = pd.concat([train_data, val_data, test_data], axis=0)\n\n# Save to CSV\ncombined_data.to_csv('/kaggle/working/data_with_scores.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T18:53:48.997437Z","iopub.execute_input":"2025-01-08T18:53:48.997749Z","iopub.status.idle":"2025-01-08T19:13:02.202166Z","shell.execute_reply.started":"2025-01-08T18:53:48.997719Z","shell.execute_reply":"2025-01-08T19:13:02.201055Z"}},"outputs":[],"execution_count":null}]}