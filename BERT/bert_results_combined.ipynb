{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bab117b8-793b-4084-971e-5dd2858a7977",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bab117b8-793b-4084-971e-5dd2858a7977",
        "outputId": "fb9a29bd-1d59-4104-d824-00b6d84de098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries if they weren't already installed\n",
        "!pip install pandas\n",
        "\n",
        "# Install necessary libraries:\n",
        "import pandas as pd\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eccafe92",
      "metadata": {
        "id": "eccafe92"
      },
      "source": [
        "# What is the sentiment of the reviews, sorted by language?\n",
        "\n",
        "We selected the Amazon Reviews Multilingual Dataset as our dataset: https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi/data. It depicts a comprehensive collection of multilingual product reviews in CSV format. This dataset contains about 1.3 million samples in 6 languages (DE = German, EN = English, ES = Spanish, FR = French, JA = Japanese, ZH = Chinese) with the following features:\n",
        "\n",
        "* **review_id**:          A string identifier of the review.\n",
        "* **product_id**:         A string identifier of the product being reviewed.\n",
        "* **reviewer_id**:        A string identifier of the reviewer.\n",
        "* **stars**:              An int between 1-5 indicating the number of stars.\n",
        "* **review_body**:        The text body of the review.\n",
        "* **review_title**:       The text title of the review.\n",
        "* **language**:           The string identifier of the review language.\n",
        "* **product_category**:   String representation of the product's category.\n",
        "\n",
        "The data will be directly downloaded and processed from the CSV files provided by the dataset. During preprocessing we will focus on removing stop words as well as choosing the correct embedding for the reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b56838a-312a-41b6-954b-39f1ac81e90d",
      "metadata": {
        "id": "5b56838a-312a-41b6-954b-39f1ac81e90d"
      },
      "source": [
        "## 7) Two new Columns:\n",
        "\n",
        "* true_label\n",
        "* predicted_label\n",
        "\n",
        "In step 4, we split train.csv that contained the initial columns:\n",
        "\n",
        "```\n",
        "review_id,product_id,reviewer_id,stars,review_body,review_title,language,product_category.\n",
        "```\n",
        "\n",
        "Based on the \"language\" (de, en, es, fr, ja, zh) of each review, we divided them into 6 seperate files so that they are seperated by language for easier ressource handling. There, we added 2 more columns for true_label and predicted_label (see step 5+6 for sentiment analysis and accuracy). Therefore, this is how a row looks like in each of the reviews_language.csv files:\n",
        "\n",
        "```\n",
        "review_id,product_id,reviewer_id,stars,review_body,review_title,language,product_category,true_label,predicted_label.\n",
        "```\n",
        "What we want to do now: take the last two columns (true_label, predicted_label) of the 6 new files and add them back to the initial big dataset (train.csv). The complete file will then contain all languages again with the two new columns additionally.\n",
        "\n",
        "We can match the rows (where each new value of true_label, predicted_label belongs to) via \"review_id\". Since they indicate the language and contain a number, this should be the easiest way to match the right values to the correct reviews. Example:\n",
        "\n",
        "```\n",
        "review_id,product_id,reviewer_id,stars,review_body,review_title,language,product_category,true_label,predicted_label.\n",
        "\n",
        "de_0232738,product_de_0901865,reviewer_de_0033281,1,Kaum Saat aufgegangen.,Schlechte Mischung,de,lawn_and_garden,0,0\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get the initial dataset:"
      ],
      "metadata": {
        "id": "paj9_sX3m26n"
      },
      "id": "paj9_sX3m26n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a2695d75-4bf1-4718-b2aa-778a77dffbe2",
      "metadata": {
        "id": "a2695d75-4bf1-4718-b2aa-778a77dffbe2"
      },
      "outputs": [],
      "source": [
        "# Read csv-file:\n",
        "file_path = 'train.csv'\n",
        "train_df = pd.read_csv(file_path) # dataframe\n",
        "\n",
        "# columns: id, review_id, product_id, reviewer_id, stars, review_body, review_title, language, product_category"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Merge files:"
      ],
      "metadata": {
        "id": "9RDHa543nvdh"
      },
      "id": "9RDHa543nvdh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The csv files contain the two new columns."
      ],
      "metadata": {
        "id": "04oxtriR1gm2"
      },
      "id": "04oxtriR1gm2"
    },
    {
      "cell_type": "code",
      "source": [
        "file_paths_languages = [\n",
        "    'german_reviews_with_sentiments_and_accuracy.csv',\n",
        "    'english_reviews_with_sentiments_and_accuracy.csv',\n",
        "    'spanish_reviews_with_sentiments_and_accuracy.csv',\n",
        "    'french_reviews_with_sentiments_and_accuracy.csv',\n",
        "    'japanese_reviews_with_sentiments_and_accuracy.csv',\n",
        "    'chinese_reviews_with_sentiments_and_accuracy.csv'\n",
        "]\n",
        "\n",
        "# init empty list\n",
        "dataframes = []\n",
        "\n",
        "# append the files to the list\n",
        "for file_path in file_paths_languages:\n",
        "    dataframes.append(pd.read_csv(file_path))\n",
        "\n",
        "# concatenate all to one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# save combined dataframe\n",
        "output_file_path = 'train_with_sentiments_and_accuracy.csv'\n",
        "combined_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Combined data saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz7HgH7kn1xc",
        "outputId": "f7348c9c-b0cb-4b7a-a7ca-85f9b82087f9"
      },
      "id": "uz7HgH7kn1xc",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined data saved to train_with_sentiments_and_accuracy.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}